if(length(performance)-which.min(performance)>patience) break
}
model$train_performance <- c(performance)
model$test_performance <-  c(performance_test)
model$weight_performance <-  c(weight_performance)
model$baseline_risk_monitor <- c(baseline_risk_monitor)
return(model)
par(mfrow=c(1,1))
model$epochs = epochs
}
model <- SRCL_1_initiate_neural_network(inputs=ncol(exposure_data), output = outcome_data,hidden=hidden_nodes)
#      for (lr_set in c(1e-4,1e-5,1e-6)) {
for (lr_set in c(1e-4)) {
print(paste0("############################## Learning rate: ",lr_set," ##############################"))
model <- SRCL_2_train_neural_network(X_train = exposure_data_train, Y_train = outcome_data_train,
X_test = exposure_data_test, Y_test = outcome_data_test,
model,lr = lr_set, epochs=1000,
patience = 500,plot_and_evaluation_frequency = 50,
L1 =0.00007) # L1 default = 0.00001
# epochs = 2000, freq = 50, patience = 100, L1 = 0.00007
}
SRCL_2_train_neural_network <- function(X_train, Y_train, X_test, Y_test, model, lr = 0.01,
epochs = 50000, patience = 500,
plot_and_evaluation_frequency = 50, IPCW = NA, L1=0.00001, spline_df=10) {
if (is.na(IPCW)) IPCW <- rep(1,nrow(X_train))
performance = model$train_performance
performance_test = model$test_performance
weight_performance = model$weight_performance
baseline_risk_monitor = model$baseline_risk_monitor
par(mfrow=c(1,3));par(mar=c(3,5,3,1))
for(rounds in 1:ceiling(c(epochs/plot_and_evaluation_frequency))) {
model <- SRCL_cpp_train_network_relu(x=as.matrix(X_train),y=as.matrix(Y_train),testx=as.matrix(X_test),testy=as.matrix(Y_test),
lr = lr, maxepochs  = plot_and_evaluation_frequency, W1_input = model[[1]],B1_input = model[[2]],
W2_input = model[[3]],B2_input = model[[4]], IPCW = IPCW, L1=L1)
performance <- c(performance,model$train_performance)
performance_test <- c(performance_test,model$test_performance)
weight_performance <- c(weight_performance,model$weight_performance)
baseline_risk_monitor <- c(baseline_risk_monitor,model$baseline_risk_monitor)
plot(performance, type='l',yaxs='i', ylab="Mean squared error",
xlab="Epochs",main="Performance on training data set")
points(smooth.spline(performance, df = spline_df),col="red",type='l')
#      plot(performance_test, type='l',yaxs='i', ylab="Mean squared error",
#           xlab="Epochs",main="Performance on test data set")
plot(log(weight_performance), type='l',yaxs='i', ylab="log of mean squared weight difference",
xlab="Epochs",main="Mean squared weight difference")
points(smooth.spline(log(weight_performance), df = spline_df),col="red",type='l')
plot(baseline_risk_monitor,type='l', main="Estimated baseline risk by epoch")
abline(h=mean(Y_train),lty=2)
points(smooth.spline(baseline_risk_monitor, df = spline_df),col="red",type='l')
if(length(performance)-which.min(performance)>patience) break
}
model$train_performance <- c(performance)
model$test_performance <-  c(performance_test)
model$weight_performance <-  c(weight_performance)
model$baseline_risk_monitor <- c(baseline_risk_monitor)
return(model)
par(mfrow=c(1,1))
model$epochs = epochs
}
model <- SRCL_1_initiate_neural_network(inputs=ncol(exposure_data), output = outcome_data,hidden=hidden_nodes)
#      for (lr_set in c(1e-4,1e-5,1e-6)) {
for (lr_set in c(1e-4)) {
print(paste0("############################## Learning rate: ",lr_set," ##############################"))
model <- SRCL_2_train_neural_network(X_train = exposure_data_train, Y_train = outcome_data_train,
X_test = exposure_data_test, Y_test = outcome_data_test,
model,lr = lr_set, epochs=1000,
patience = 500,plot_and_evaluation_frequency = 50,
L1 =0.00007) # L1 default = 0.00001
# epochs = 2000, freq = 50, patience = 100, L1 = 0.00007
}
#      for (lr_set in c(1e-4,1e-5,1e-6)) {
for (lr_set in c(1e-4)) {
print(paste0("############################## Learning rate: ",lr_set," ##############################"))
model <- SRCL_2_train_neural_network(X_train = exposure_data_train, Y_train = outcome_data_train,
X_test = exposure_data_test, Y_test = outcome_data_test,
model,lr = lr_set, epochs=1000,
patience = 500,plot_and_evaluation_frequency = 50,
L1 =0.00007) # L1 default = 0.00001
# epochs = 2000, freq = 50, patience = 100, L1 = 0.00007
}
#      for (lr_set in c(1e-4,1e-5,1e-6)) {
for (lr_set in c(1e-4)) {
print(paste0("############################## Learning rate: ",lr_set," ##############################"))
model <- SRCL_2_train_neural_network(X_train = exposure_data_train, Y_train = outcome_data_train,
X_test = exposure_data_test, Y_test = outcome_data_test,
model,lr = lr_set, epochs=1000,
patience = 500,plot_and_evaluation_frequency = 50,
L1 =0.00007) # L1 default = 0.00001
# epochs = 2000, freq = 50, patience = 100, L1 = 0.00007
}
for (lr_set in c(1e-4,1e-5,1e-6)) {
#      for (lr_set in c(1e-4)) {
print(paste0("############################## Learning rate: ",lr_set," ##############################"))
model <- SRCL_2_train_neural_network(X_train = exposure_data_train, Y_train = outcome_data_train,
X_test = exposure_data_test, Y_test = outcome_data_test,
model,lr = lr_set, epochs=1000,
patience = 500,plot_and_evaluation_frequency = 50,
L1 =0.00007) # L1 default = 0.00001
# epochs = 2000, freq = 50, patience = 100, L1 = 0.00007
}
model <- SRCL_1_initiate_neural_network(inputs=ncol(exposure_data), output = outcome_data,hidden=hidden_nodes)
for (lr_set in c(1e-4,1e-5,1e-6)) {
#      for (lr_set in c(1e-4)) {
print(paste0("############################## Learning rate: ",lr_set," ##############################"))
model <- SRCL_2_train_neural_network(X_train = exposure_data_train, Y_train = outcome_data_train,
X_test = exposure_data_test, Y_test = outcome_data_test,
model,lr = lr_set, epochs=1000,
patience = 100,plot_and_evaluation_frequency = 50,
L1 =0.00007) # L1 default = 0.00001
# epochs = 2000, freq = 50, patience = 100, L1 = 0.00007
}
SRCL_2_train_neural_network <- function(X_train, Y_train, X_test, Y_test, model, lr = 0.01,
epochs = 50000, patience = 500,
plot_and_evaluation_frequency = 50, IPCW = NA, L1=0.00001, spline_df=10) {
if (is.na(IPCW)) IPCW <- rep(1,nrow(X_train))
performance = model$train_performance
performance_test = model$test_performance
weight_performance = model$weight_performance
baseline_risk_monitor = model$baseline_risk_monitor
par(mfrow=c(1,3));par(mar=c(3,5,3,1))
for(rounds in 1:ceiling(c(epochs/plot_and_evaluation_frequency))) {
model <- SRCL_cpp_train_network_relu(x=as.matrix(X_train),y=as.matrix(Y_train),testx=as.matrix(X_test),testy=as.matrix(Y_test),
lr = lr, maxepochs  = plot_and_evaluation_frequency, W1_input = model[[1]],B1_input = model[[2]],
W2_input = model[[3]],B2_input = model[[4]], IPCW = IPCW, L1=L1)
performance <- c(performance,model$train_performance)
performance_test <- c(performance_test,model$test_performance)
weight_performance <- c(weight_performance,model$weight_performance)
baseline_risk_monitor <- c(baseline_risk_monitor,model$baseline_risk_monitor)
plot(log(performance), type='l',yaxs='i', ylab="Log mean squared error",
xlab="Epochs",main="Log performance on training data set")
points(smooth.spline(log(performance), df = spline_df),col="red",type='l')
#      plot(performance_test, type='l',yaxs='i', ylab="Mean squared error",
#           xlab="Epochs",main="Performance on test data set")
plot(log(weight_performance), type='l',yaxs='i', ylab="log of mean squared weight difference",
xlab="Epochs",main="Log mean squared weight difference")
points(smooth.spline(log(weight_performance), df = spline_df),col="red",type='l')
plot(baseline_risk_monitor,type='l', main="Estimated baseline risk by epoch")
abline(h=mean(Y_train),lty=2)
points(smooth.spline(baseline_risk_monitor, df = spline_df),col="red",type='l')
if(length(performance)-which.min(performance)>patience) break
}
model$train_performance <- c(performance)
model$test_performance <-  c(performance_test)
model$weight_performance <-  c(weight_performance)
model$baseline_risk_monitor <- c(baseline_risk_monitor)
return(model)
par(mfrow=c(1,1))
model$epochs = epochs
}
model <- SRCL_1_initiate_neural_network(inputs=ncol(exposure_data), output = outcome_data,hidden=hidden_nodes)
for (lr_set in c(1e-4,1e-5,1e-6)) {
#      for (lr_set in c(1e-4)) {
print(paste0("############################## Learning rate: ",lr_set," ##############################"))
model <- SRCL_2_train_neural_network(X_train = exposure_data_train, Y_train = outcome_data_train,
X_test = exposure_data_test, Y_test = outcome_data_test,
model,lr = lr_set, epochs=1000,
patience = 100,plot_and_evaluation_frequency = 50,
L1 =0.00007) # L1 default = 0.00001
# epochs = 2000, freq = 50, patience = 100, L1 = 0.00007
}
SRCL_2_train_neural_network <- function(X_train, Y_train, X_test, Y_test, model, lr = 0.01,
epochs = 50000, patience = 500,
plot_and_evaluation_frequency = 50, IPCW = NA, L1=0.00001, spline_df=10) {
if (is.na(IPCW)) IPCW <- rep(1,nrow(X_train))
performance = model$train_performance
performance_test = model$test_performance
weight_performance = model$weight_performance
baseline_risk_monitor = model$baseline_risk_monitor
par(mfrow=c(1,3));par(mar=c(3,5,3,1))
for(rounds in 1:ceiling(c(epochs/plot_and_evaluation_frequency))) {
model <- SRCL_cpp_train_network_relu(x=as.matrix(X_train),y=as.matrix(Y_train),testx=as.matrix(X_test),testy=as.matrix(Y_test),
lr = lr, maxepochs  = plot_and_evaluation_frequency, W1_input = model[[1]],B1_input = model[[2]],
W2_input = model[[3]],B2_input = model[[4]], IPCW = IPCW, L1=L1)
performance <- c(performance,model$train_performance)
performance_test <- c(performance_test,model$test_performance)
weight_performance <- c(weight_performance,model$weight_performance)
baseline_risk_monitor <- c(baseline_risk_monitor,model$baseline_risk_monitor)
plot(performance, type='l',yaxs='i', ylab="Mean squared error",
xlab="Epochs",main="Performance on training data set")
points(smooth.spline(performance, df = spline_df),col="red",type='l')
#      plot(performance_test, type='l',yaxs='i', ylab="Mean squared error",
#           xlab="Epochs",main="Performance on test data set")
plot(log(weight_performance), type='l',yaxs='i', ylab="log of mean squared weight difference",
xlab="Epochs",main="Log mean squared weight difference")
points(smooth.spline(log(weight_performance), df = spline_df),col="red",type='l')
plot(baseline_risk_monitor,type='l', main="Estimated baseline risk by epoch")
abline(h=mean(Y_train),lty=2)
points(smooth.spline(baseline_risk_monitor, df = spline_df),col="red",type='l')
if(length(performance)-which.min(performance)>patience) break
}
model$train_performance <- c(performance)
model$test_performance <-  c(performance_test)
model$weight_performance <-  c(weight_performance)
model$baseline_risk_monitor <- c(baseline_risk_monitor)
return(model)
par(mfrow=c(1,1))
model$epochs = epochs
}
model <- SRCL_1_initiate_neural_network(inputs=ncol(exposure_data), output = outcome_data,hidden=hidden_nodes)
for (lr_set in c(1e-4,1e-5,1e-6)) {
#      for (lr_set in c(1e-4)) {
print(paste0("############################## Learning rate: ",lr_set," ##############################"))
model <- SRCL_2_train_neural_network(X_train = exposure_data_train, Y_train = outcome_data_train,
X_test = exposure_data_test, Y_test = outcome_data_test,
model,lr = lr_set, epochs=1000,
patience = 100,plot_and_evaluation_frequency = 50,
L1 =0.00007) # L1 default = 0.00001
# epochs = 2000, freq = 50, patience = 100, L1 = 0.00007
}
SRCL_2_train_neural_network <- function(X_train, Y_train, X_test, Y_test, model, lr = 0.01,
epochs = 50000, patience = 500,
plot_and_evaluation_frequency = 50, IPCW = NA, L1=0.00001, spline_df=10) {
if (is.na(IPCW)) IPCW <- rep(1,nrow(X_train))
performance = model$train_performance
performance_test = model$test_performance
weight_performance = model$weight_performance
baseline_risk_monitor = model$baseline_risk_monitor
par(mfrow=c(1,3));par(mar=c(3,5,3,1))
for(rounds in 1:ceiling(c(epochs/plot_and_evaluation_frequency))) {
model <- SRCL_cpp_train_network_relu(x=as.matrix(X_train),y=as.matrix(Y_train),testx=as.matrix(X_test),testy=as.matrix(Y_test),
lr = lr, maxepochs  = plot_and_evaluation_frequency, W1_input = model[[1]],B1_input = model[[2]],
W2_input = model[[3]],B2_input = model[[4]], IPCW = IPCW, L1=L1)
performance <- c(performance,model$train_performance)
performance_test <- c(performance_test,model$test_performance)
weight_performance <- c(weight_performance,model$weight_performance)
baseline_risk_monitor <- c(baseline_risk_monitor,model$baseline_risk_monitor)
plot(performance, type='l', ylab="Mean squared error",
xlab="Epochs",main="Performance on training data set")
points(smooth.spline(performance, df = spline_df),col="red",type='l')
#      plot(performance_test, type='l',yaxs='i', ylab="Mean squared error",
#           xlab="Epochs",main="Performance on test data set")
plot(log(weight_performance), type='l', ylab="log of mean squared weight difference",
xlab="Epochs",main="Log mean squared weight difference")
points(smooth.spline(log(weight_performance), df = spline_df),col="red",type='l')
plot(baseline_risk_monitor,type='l', main="Estimated baseline risk by epoch")
abline(h=mean(Y_train),lty=2)
points(smooth.spline(baseline_risk_monitor, df = spline_df),col="red",type='l')
if(length(performance)-which.min(performance)>patience) break
}
model$train_performance <- c(performance)
model$test_performance <-  c(performance_test)
model$weight_performance <-  c(weight_performance)
model$baseline_risk_monitor <- c(baseline_risk_monitor)
return(model)
par(mfrow=c(1,1))
model$epochs = epochs
}
model <- SRCL_1_initiate_neural_network(inputs=ncol(exposure_data), output = outcome_data,hidden=hidden_nodes)
for (lr_set in c(1e-4,1e-5,1e-6)) {
#      for (lr_set in c(1e-4)) {
print(paste0("############################## Learning rate: ",lr_set," ##############################"))
model <- SRCL_2_train_neural_network(X_train = exposure_data_train, Y_train = outcome_data_train,
X_test = exposure_data_test, Y_test = outcome_data_test,
model,lr = lr_set, epochs=1000,
patience = 100,plot_and_evaluation_frequency = 50,
L1 =0.00007) # L1 default = 0.00001
# epochs = 2000, freq = 50, patience = 100, L1 = 0.00007
}
SRCL_2_train_neural_network <- function(X_train, Y_train, X_test, Y_test, model, lr = 0.01,
epochs = 50000, patience = 500,
plot_and_evaluation_frequency = 50, IPCW = NA, L1=0.00001, spline_df=10) {
if (is.na(IPCW)) IPCW <- rep(1,nrow(X_train))
performance = model$train_performance
performance_test = model$test_performance
weight_performance = model$weight_performance
baseline_risk_monitor = model$baseline_risk_monitor
par(mfrow=c(1,3));par(mar=c(3,5,3,1))
for(rounds in 1:ceiling(c(epochs/plot_and_evaluation_frequency))) {
model <- SRCL_cpp_train_network_relu(x=as.matrix(X_train),y=as.matrix(Y_train),testx=as.matrix(X_test),testy=as.matrix(Y_test),
lr = lr, maxepochs  = plot_and_evaluation_frequency, W1_input = model[[1]],B1_input = model[[2]],
W2_input = model[[3]],B2_input = model[[4]], IPCW = IPCW, L1=L1)
performance <- c(performance,model$train_performance)
performance_test <- c(performance_test,model$test_performance)
weight_performance <- c(weight_performance,model$weight_performance)
baseline_risk_monitor <- c(baseline_risk_monitor,model$baseline_risk_monitor)
plot(performance, type='l',yaxs='i', ylab="Mean squared error",
xlab="Epochs",main="Performance on training data set")
points(smooth.spline(performance, df = spline_df),col="red",type='l',lwd=2)
#      plot(performance_test, type='l',yaxs='i', ylab="Mean squared error",
#           xlab="Epochs",main="Performance on test data set")
plot(log(weight_performance), type='l', ylab="log of mean squared weight difference",
xlab="Epochs",main="Log mean squared weight difference")
points(smooth.spline(log(weight_performance), df = spline_df),col="red",type='l',lwd=2)
plot(baseline_risk_monitor,type='l', main="Estimated baseline risk by epoch")
abline(h=mean(Y_train),lty=2)
points(smooth.spline(baseline_risk_monitor, df = spline_df),col="red",type='l',lwd=2)
if(length(performance)-which.min(performance)>patience) break
}
model$train_performance <- c(performance)
model$test_performance <-  c(performance_test)
model$weight_performance <-  c(weight_performance)
model$baseline_risk_monitor <- c(baseline_risk_monitor)
return(model)
par(mfrow=c(1,1))
model$epochs = epochs
}
model <- SRCL_1_initiate_neural_network(inputs=ncol(exposure_data), output = outcome_data,hidden=hidden_nodes)
for (lr_set in c(1e-4,1e-5,1e-6)) {
#      for (lr_set in c(1e-4)) {
print(paste0("############################## Learning rate: ",lr_set," ##############################"))
model <- SRCL_2_train_neural_network(X_train = exposure_data_train, Y_train = outcome_data_train,
X_test = exposure_data_test, Y_test = outcome_data_test,
model,lr = lr_set, epochs=1000,
patience = 100,plot_and_evaluation_frequency = 50,
L1 =0.00007) # L1 default = 0.00001
# epochs = 2000, freq = 50, patience = 100, L1 = 0.00007
}
# Risk contributions
r_c <- SRCL_5_layerwise_relevance_propagation(exposure_data,model)
sum(duplicated(r_c)==FALSE)
# Correct weighting using hclustgeo
# Clustering - using aggregated data - USED - but seems to fail witn noise data.
groups = 3
library(fastcluster)
library(ClustGeo)
p <- cbind(r_c)
p <- plyr::count(p)
pfreq <- p$freq
p <- p[,-c(ncol(p))]
p_h_c <- hclustgeo(dist(p,method = "manhattan"), wt=pfreq)
#p_h_c <- hclust(dist(p, method = "manhattan"),method = "ward.D2", members=pfreq)
pclus <- cutree(p_h_c, groups)
id <- 1:nrow(r_c)
temp <- merge(cbind(id,r_c),cbind(p,pclus))
clus <- temp$pclus[order(temp$id)]
table(clus)
par(mfrow=c(1,1))
par(mar=c(5,5,5,5))
library(ggtree)
library(ggplot2)
print(ggtree(p_h_c,layout="equal_angle") +
geom_tippoint(size=sqrt(pfreq)/2, alpha=.2, color=colours[pclus])+
ggtitle("D) Dendrogram") +
theme(plot.title = element_text(size = 15, face = "bold")))
layout(matrix(c(1,1,2,2,3,3,4,4,4,5,5,5,6,6,6,6,6,6), 3, 6, byrow = TRUE))
# Performance - trian
par(mar=c(5,5,2,0))
plot(model$train_performance, type='l',yaxs='i', ylab="Mean squared error",
xlab="Epochs",main="A) Performance - training data")
# Model visualisation
par(mar=c(0,0,0,0))
SRCL_3_plot_neural_network(model,names(exposure_data),5/max(model[[1]]), title = "B) Model")
# AUC
library(pROC)
par(mar=c(5,5,2,0))
pred <- SRCL_4_predict_risks(exposure_data,model)
plot(roc(outcome_data,pred),print.auc=TRUE,main="C) Accuracy")
# Plot results
library(robustbase)
library(imager)
im <- load.image("dendrogram.png")
par(mar=c(0,0,0,0))
plot(im,axes=F)
# par(mar=c(4,5,3,0))
# plot(prcomp(r_c),main="PCA: Proportion of variance")
# plot(prcomp(r_c)$x[,1:2],pch=16,col=colours[clus],main="PCA: Biplot",frame.plot=FALSE)
par(mar=c(4,5,2,1))
plot(0,0,type='n',xlim=c(0,1),ylim=c(0,max(pred)),xaxs='i',yaxs='i',
axes=FALSE,ylab="Risk",xlab="Prevalence",frame.plot=FALSE,main="E) Prevalence and mean risk\nof sub-groups")
axis(1,seq(0,1,.2));axis(2,seq(0,1,.05))
rect(0,0,1,1)
prev0 = 0; total = 0
for (i in 1:groups) {
prev <- sum(clus==i)/length(clus)
#  risk <- sum(colMedians(as.matrix(r_c[clus==i,])))
risk <- sum(colMeans(as.matrix(r_c[clus==i,])))
rect(xleft = prev0,ybottom = 0,xright = prev+prev0,ytop = risk, col=colours[i])
prev0 = prev + prev0
total = total + risk * prev
}
#arrows(x0=0,x1=1,y0=median(r_c$Baseline_risk),lty=1,length=0)
arrows(x0=0,x1=1,y0=mean(r_c$Baseline_risk),lty=1,length=0)
st <- 1
d <- data.frame(matrix(NA, nrow=ncol(r_c)))
for (g in 1:groups) {
for (i in 1:nrow(d)) {
#    d[i,g] <- median(r_c[clus==g,i])
d[i,g] <- mean(r_c[clus==g,i])
}}
d <- t(d)
rownames(d) <- paste("Group",1:groups)
colnames(d) <- names(r_c)
par(mar=c(0,0,0,0))
plot(0,0,type='n',xlim=c(-ncol(d)-6,0),ylim=c(-nrow(d)-1,1),axes=F)
text(c(-ncol(d)):c(-1),0,rev(colnames(d)),srt=25,cex=st)
text(-ncol(d)-6,0,"F) Mean risk contributions by sub-group (SD)\n[mean risk contribution if other exposures are set to 0]",pos=4,cex=st)
#text(-ncol(d)-2,c(-1):c(-nrow(d)),rownames(d),col=colours[1:groups],cex=st)
#par(mfrow=c(1,1))
for (i in 1:groups) {
prev <- sum(clus==i)/length(clus)
#  risk <- sum(colMedians(as.matrix(r_c[clus==i,])))
risk <- sum(colMeans(as.matrix(r_c[clus==i,])))
risk_obs <- mean(outcome_data[clus==i])
text(-ncol(d)-6,-i,paste0("Sub-group ",i,": ","n=",sum(clus==i),", e=",sum(outcome_data[clus==i]),",Prev=",format(round(prev*100,1),nsmall=1),"%, risk=",format(round(risk*100,1),nsmall=1),"%,\nexcess=",
#round(prev*(risk-median(r_c$Baseline_risk))/total*100,1),
format(round(prev*(risk-mean(r_c$Baseline_risk))/total*100,1),nsmall=1),
"%, Obs risk=",format(round(risk_obs*100,1),nsmall=1),"% (",
#                     "round(risk_obs*100),
paste0(format(round(prop.test(sum(outcome_data[clus==i]),length(t(outcome_data)[clus==i]))$conf.int*100,1),nsmall=1),collapse="-"),
"%)\n",
"Risk based on the sum of individual effects =",
format(round(mean(SRCL_6_sum_of_individual_effects(exposure_data,model)[clus==i])*100,1),nsmall=1),
"%"),pos=4,col=colours[i])
#  ex_mat[run,i] <- prev*(risk-mean(r_c$Baseline_risk))/total*100
}
m <- max(d)
ind_effect_matrix <- SRCL_6_individual_effects_matrix(exposure_data,model)
for(g in 1:ncol(d)) { for (i in 1:nrow(d)){
value <- paste0(format(round(as.numeric(d[i,g])*100,1),nsmall=),"%\n(",
format(round(sd(r_c[clus==i,g])*100,1),nsmall=1),"%)\n[",
format(round(mean(ind_effect_matrix[clus==i,g]*100),1),nsmall=1),"%]"
)
#  value <- paste0(format(round(as.numeric(quantile(d[i,g],c(0.25,0.75))),2),nsmall=2),collapse = "-")
text(-g,-i,value,col=adjustcolor(colours[i],d[i,g]/m),cex=st*d[i,g]/m)
#text(-g,-i,value,col=adjustcolor(ifelse(d[i,g] <0.005,"white",colours[i]),1),cex=st)
}}
dev.off()
layout(matrix(c(1,1,2,2,3,3,4,4,4,5,5,5,6,6,6,6,6,6), 3, 6, byrow = TRUE))
# Performance - trian
par(mar=c(5,5,2,0))
plot(model$train_performance, type='l',yaxs='i', ylab="Mean squared error",
xlab="Epochs",main="A) Performance - training data")
# Performance - test
#      par(mar=c(5,5,2,0))
#      plot(model$test_performance, type='l',yaxs='i', ylab="Mean squared error",
#           xlab="Epochs",main="A) Performance - test")
# Model visualisation
par(mar=c(0,0,0,0))
SRCL_3_plot_neural_network(model,names(exposure_data),5/max(model[[1]]), title = "B) Model")
# AUC
library(pROC)
par(mar=c(5,5,2,0))
pred <- SRCL_4_predict_risks(exposure_data,model)
plot(roc(outcome_data,pred),print.auc=TRUE,main="C) Accuracy")
# Plot results
library(robustbase)
library(imager)
im <- load.image("dendrogram.png")
par(mar=c(0,0,0,0))
plot(im,axes=F)
# par(mar=c(4,5,3,0))
# plot(prcomp(r_c),main="PCA: Proportion of variance")
# plot(prcomp(r_c)$x[,1:2],pch=16,col=colours[clus],main="PCA: Biplot",frame.plot=FALSE)
par(mar=c(4,5,2,1))
plot(0,0,type='n',xlim=c(0,1),ylim=c(0,max(pred)),xaxs='i',yaxs='i',
axes=FALSE,ylab="Risk",xlab="Prevalence",frame.plot=FALSE,main="E) Prevalence and mean risk\nof sub-groups")
axis(1,seq(0,1,.2));axis(2,seq(0,1,.05))
rect(0,0,1,1)
prev0 = 0; total = 0
for (i in 1:groups) {
prev <- sum(clus==i)/length(clus)
#  risk <- sum(colMedians(as.matrix(r_c[clus==i,])))
risk <- sum(colMeans(as.matrix(r_c[clus==i,])))
rect(xleft = prev0,ybottom = 0,xright = prev+prev0,ytop = risk, col=colours[i])
prev0 = prev + prev0
total = total + risk * prev
}
#arrows(x0=0,x1=1,y0=median(r_c$Baseline_risk),lty=1,length=0)
arrows(x0=0,x1=1,y0=mean(r_c$Baseline_risk),lty=1,length=0)
st <- 1
d <- data.frame(matrix(NA, nrow=ncol(r_c)))
for (g in 1:groups) {
for (i in 1:nrow(d)) {
#    d[i,g] <- median(r_c[clus==g,i])
d[i,g] <- mean(r_c[clus==g,i])
}}
d <- t(d)
rownames(d) <- paste("Group",1:groups)
colnames(d) <- names(r_c)
par(mar=c(0,0,0,0))
plot(0,0,type='n',xlim=c(-ncol(d)-6,0),ylim=c(-nrow(d)-1,1),axes=F)
text(c(-ncol(d)):c(-1),0,rev(colnames(d)),srt=25,cex=st)
text(-ncol(d)-6,0,"F) Mean risk contributions by sub-group (SD)\n[mean risk contribution if other exposures are set to 0]",pos=4,cex=st)
#text(-ncol(d)-2,c(-1):c(-nrow(d)),rownames(d),col=colours[1:groups],cex=st)
#par(mfrow=c(1,1))
for (i in 1:groups) {
prev <- sum(clus==i)/length(clus)
#  risk <- sum(colMedians(as.matrix(r_c[clus==i,])))
risk <- sum(colMeans(as.matrix(r_c[clus==i,])))
risk_obs <- mean(outcome_data[clus==i])
text(-ncol(d)-6,-i,paste0("Sub-group ",i,": ","n=",sum(clus==i),", e=",sum(outcome_data[clus==i]),",Prev=",format(round(prev*100,1),nsmall=1),"%, risk=",format(round(risk*100,1),nsmall=1),"%,\nexcess=",
#round(prev*(risk-median(r_c$Baseline_risk))/total*100,1),
format(round(prev*(risk-mean(r_c$Baseline_risk))/total*100,1),nsmall=1),
"%, Obs risk=",format(round(risk_obs*100,1),nsmall=1),"% (",
#                     "round(risk_obs*100),
paste0(format(round(prop.test(sum(outcome_data[clus==i]),length(t(outcome_data)[clus==i]))$conf.int*100,1),nsmall=1),collapse="-"),
"%)\n",
"Risk based on the sum of individual effects =",
format(round(mean(SRCL_6_sum_of_individual_effects(exposure_data,model)[clus==i])*100,1),nsmall=1),
"%"),pos=4,col=colours[i])
#  ex_mat[run,i] <- prev*(risk-mean(r_c$Baseline_risk))/total*100
}
m <- max(d)
ind_effect_matrix <- SRCL_6_individual_effects_matrix(exposure_data,model)
for(g in 1:ncol(d)) { for (i in 1:nrow(d)){
value <- paste0(format(round(as.numeric(d[i,g])*100,1),nsmall=),"%\n(",
format(round(sd(r_c[clus==i,g])*100,1),nsmall=1),"%)\n[",
format(round(mean(ind_effect_matrix[clus==i,g]*100),1),nsmall=1),"%]"
)
#  value <- paste0(format(round(as.numeric(quantile(d[i,g],c(0.25,0.75))),2),nsmall=2),collapse = "-")
text(-g,-i,value,col=adjustcolor(colours[i],d[i,g]/m),cex=st*d[i,g]/m)
#text(-g,-i,value,col=adjustcolor(ifelse(d[i,g] <0.005,"white",colours[i]),1),cex=st)
}}
#   mtext(paste0("SG1= ",round(mean(data$Y[data$Air_pollution==1&data$Mutation_X==1])*100,1),"%, ",
#                 "SG2= ",round(mean(data$Y[data$Physically_active==0&data$LDL==1&data$Night_shifts==1])*100,1),"%"),side=1,line=-1,cex=.7)
library(SRCL)
library(CoOL)
library(CoOL)
library(CoOL)
#colours <- c("grey","dodgerblue","red","orange","green","yellow","violet")
colours <- c("grey",wes_palette("Darjeeling1"))
remove.packages("CoOL", lib="~/R/win-library/4.0")
